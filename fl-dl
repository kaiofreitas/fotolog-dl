#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
So, fotolog.com is coming to an end... and we want our photos..
Script usage

First:
    - pip install -r requirements.txt
Then:
    - ./fl-dl YOUR_FOTOLOG_USERNAME SAVE_IMAGE_PATH
    - Enjoy.
"""
import os
import time
import uuid
from zipfile import ZipFile
import asyncio

import click
from robobrowser import RoboBrowser
import uvloop
import aiohttp


async def download_img(link, path):
    print("downloading -> {}".format(link))
    r = await aiohttp.request('GET', link)
    image = await r.read()

    # saving
    if r.status == 200:
        id_ = str(uuid.uuid4()).replace('-', '')
        filename = os.path.join(path, 'image_{}.jpg'.format(id_))
        with open(filename, 'wb') as im:
            im.write(image)
        return filename
    return False


def download_images(links, path):
    loop = uvloop.new_event_loop()
    asyncio.set_event_loop(loop)
    futures = [download_img(link, path) for link in links]
    task = asyncio.wait(futures)
    done, _ = loop.run_until_complete(task)
    loop.close()
    images = [i.result() for i in done]
    return images


@click.command()
@click.argument('username')
@click.argument('path', default='.')
def main(username, path):
    t0 = time.time()
    fotolog_url = 'http://www.fotolog.com/{}/mosaic/'.format(username)
    per_page = 30
    all_links = []

    browser = RoboBrowser(history=True, parser='html.parser')
    browser.open(fotolog_url)

    # get last page
    pagination_links = browser.find(id='pagination').children

    for item in pagination_links:
        pass

    links = browser.find_all('a', class_='wall_img_container')

    page = 1
    while links:
        click.secho('Fetching page {}...'.format(page), fg='yellow')

        # if the page > last page we get redirected back to the original one
        # so we need to stop at this point
        if browser.response.url == fotolog_url and page > 1:
            break

        for link in links:
            browser.follow_link(link)
            try:
                img = browser.find('a',
                        class_='wall_img_container_big').next_element
            except:
                click.echo('img not found, skipping')
                continue

            img_url = img.attrs['src']
            all_links.append(img_url)
            browser.back()
        page += 1

        new_page_url = '{}{}'.format(fotolog_url, per_page * page)
        click.echo('Opening {}'.format(new_page_url))
        browser.open(new_page_url)
        links = browser.find_all('a', class_='wall_img_container')

    saved_images = download_images(all_links, path)
    errors = len(list(filter(lambda x: x is False, saved_images)))

    # zip all the things
    saved = 0
    zip_file = os.path.join(path, '{}_photos.zip'.format(username))
    with ZipFile(zip_file, 'w') as z:
        for s in saved_images:
            if not s:
                continue
            z.write(s)
            os.remove(os.path.join(path, s))
            saved += 1
    t1 = time.time()
    print(click.secho("""
Images saved..
{} Images saved.
{} Images couldn't be downloaded.

Execution time: {:.2f} seconds
""".format(saved, 0, (t1 - t0)), fg='green', bold=True))

if __name__ == "__main__":
    main()
